{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTXVLAEJZiLmRl2/bkUmWt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KavinduPiyumantha/ComposeBottomSheetIssue/blob/master/Open_AI_Fine_Tune_GoogleSheet-to-JSONL_Convert_Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Training data - Neluni\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jCft53dAZ3qo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install necessary libraries\n",
        "!pip install --upgrade gspread pandas gspread-dataframe tiktoken pytz --quiet\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "import gspread\n",
        "from google.auth.transport.requests import Request\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import pandas as pd\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "\n",
        "# Step 3: Authenticate and create the gspread client\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "creds.refresh(Request())\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "# Step 4: Open the Google Sheet by URL\n",
        "sheet = gc.open_by_url('https://docs.google.com/spreadsheets/d/14eO9Gyo6vjE6hulXd72dyXx8rLFlaI9NACzLlisJAlM/edit?usp=sharing')\n",
        "worksheet = sheet.get_worksheet(0)  # Open the first sheet\n",
        "\n",
        "# Step 5: Read data from Google Sheet into pandas DataFrame\n",
        "data = worksheet.get_all_records()\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Print the column names to debug\n",
        "print(\"Column names:\", df.columns)\n",
        "\n",
        "# New system prompt\n",
        "system_prompt = (\"You are a subject matter expert creating a test for the purpose of pre-employment screening for a variety of roles and positions. \"\n",
        "                 \"Your task is to create highly relevant technical and non-technical questions to evaluate the candidates’ skills in a given subject. \"\n",
        "                 \"This is a test and needs to be marked for their overall score, therefore ask questions with a clear correct answer. \"\n",
        "                 \"Do not ask questions that ask the candidate to rate themselves, as this will skew the marking criteria. \"\n",
        "                 \"Ask questions that have a correct answer or logical questions that can be evaluated. \"\n",
        "                 \"Taylor the test to the amount of questions required always provided the required number of questions eg: if requested 30 questions provide 30 question’s and 30 answers. \"\n",
        "                 \"Create questions along with their correct answers for marking.\")\n",
        "\n",
        "# Function to convert a row of the DataFrame to a JSON object for OpenAI fine-tuning\n",
        "def row_to_json(row):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Role: {row['Role']}\"},\n",
        "        {\"role\": \"assistant\", \"content\": f\"Question: {row['Question']} Answer: {row['Answer']}\"}\n",
        "    ]\n",
        "    return {\"messages\": messages}\n",
        "\n",
        "# Convert the entire DataFrame to a list of JSON objects\n",
        "jsonl_data = df.apply(row_to_json, axis=1).tolist()\n",
        "\n",
        "# Step 6: Format Error Checks Function\n",
        "def check_format_errors(dataset):\n",
        "    format_errors = defaultdict(int)\n",
        "    for ex in dataset:\n",
        "        if not isinstance(ex, dict):\n",
        "            format_errors[\"data_type\"] += 1\n",
        "            continue\n",
        "\n",
        "        messages = ex.get(\"messages\", None)\n",
        "        if not messages:\n",
        "            format_errors[\"missing_messages_list\"] += 1\n",
        "            continue\n",
        "\n",
        "        for message in messages:\n",
        "            if \"role\" not in message or \"content\" not in message:\n",
        "                format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "            if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
        "                format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "            if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
        "                format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "            content = message.get(\"content\", None)\n",
        "            function_call = message.get(\"function_call\", None)\n",
        "\n",
        "            if (not content and not function_call) or not isinstance(content, str):\n",
        "                format_errors[\"missing_content\"] += 1\n",
        "\n",
        "        if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "            format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "    if format_errors:\n",
        "        print(\"Found errors:\")\n",
        "        for k, v in format_errors.items():\n",
        "            print(f\"{k}: {v}\")\n",
        "    else:\n",
        "        print(\"No errors found\")\n",
        "\n",
        "# Step 7: Token Counting Utilities\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribution of {name}:\")\n",
        "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
        "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
        "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
        "\n",
        "# Step 8: Data Warnings and Token Counts Function\n",
        "def data_warnings_and_token_counts(dataset):\n",
        "    n_missing_system = 0\n",
        "    n_missing_user = 0\n",
        "    n_messages = []\n",
        "    convo_lens = []\n",
        "    assistant_message_lens = []\n",
        "\n",
        "    for ex in dataset:\n",
        "        messages = ex[\"messages\"]\n",
        "        if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "            n_missing_system += 1\n",
        "        if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "            n_missing_user += 1\n",
        "        n_messages.append(len(messages))\n",
        "        convo_lens.append(num_tokens_from_messages(messages))\n",
        "        assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "\n",
        "    print(\"Num examples missing system message:\", n_missing_system)\n",
        "    print(\"Num examples missing user message:\", n_missing_user)\n",
        "    print_distribution(n_messages, \"num_messages_per_example\")\n",
        "    print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "    print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "    n_too_long = sum(l > 16385 for l in convo_lens)\n",
        "    print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")\n",
        "\n",
        "# Step 9: Cost Estimation Function\n",
        "def estimate_costs(dataset):\n",
        "    MAX_TOKENS_PER_EXAMPLE = 16385\n",
        "    TARGET_EPOCHS = 3\n",
        "    MIN_TARGET_EXAMPLES = 100\n",
        "    MAX_TARGET_EXAMPLES = 25000\n",
        "    MIN_DEFAULT_EPOCHS = 1\n",
        "    MAX_DEFAULT_EPOCHS = 25\n",
        "\n",
        "    n_epochs = TARGET_EPOCHS\n",
        "    n_train_examples = len(dataset)\n",
        "    if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "        n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "    elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "        n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "    convo_lens = [num_tokens_from_messages(ex[\"messages\"]) for ex in dataset]\n",
        "    n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "    print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
        "    print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "    print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
        "\n",
        "# Step 10: Execute the functions with the dataset\n",
        "check_format_errors(jsonl_data)\n",
        "data_warnings_and_token_counts(jsonl_data)\n",
        "estimate_costs(jsonl_data)\n",
        "\n",
        "# Step 11: Save the JSONL data to a file with date and time in the filename\n",
        "australia_timezone = pytz.timezone('Australia/Sydney')\n",
        "current_datetime = datetime.now(australia_timezone).strftime('%Y-%m-%d_%H-%M-%S')\n",
        "output_file = f'fine_tuning_data_{current_datetime}.jsonl'\n",
        "\n",
        "with open(output_file, 'w') as f:\n",
        "    for entry in jsonl_data:\n",
        "        json.dump(entry, f)\n",
        "        f.write('\\n')\n",
        "\n",
        "print(f'Data has been successfully saved to {output_file}')\n",
        "\n",
        "# Optional: Download the JSONL file to your local machine\n",
        "from google.colab import files\n",
        "files.download(output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "uaEh7X6m4fZ3",
        "outputId": "16591ebd-90ab-4458-c5bb-2c79b7ab2920"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.1.4, but you have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mColumn names: Index(['Role', 'Difficulty', 'Question', 'Answer'], dtype='object')\n",
            "No errors found\n",
            "Num examples missing system message: 0\n",
            "Num examples missing user message: 0\n",
            "\n",
            "#### Distribution of num_messages_per_example:\n",
            "min / max: 3, 3\n",
            "mean / median: 3.0, 3.0\n",
            "p5 / p95: 3.0, 3.0\n",
            "\n",
            "#### Distribution of num_total_tokens_per_example:\n",
            "min / max: 201, 433\n",
            "mean / median: 266.55555555555554, 265.0\n",
            "p5 / p95: 207.0, 335.2\n",
            "\n",
            "#### Distribution of num_assistant_tokens_per_example:\n",
            "min / max: 32, 264\n",
            "mean / median: 97.16161616161617, 96.0\n",
            "p5 / p95: 38.0, 164.0\n",
            "\n",
            "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n",
            "Dataset has ~26389 tokens that will be charged for during training\n",
            "By default, you'll train for 3 epochs on this dataset\n",
            "By default, you'll be charged for ~79167 tokens\n",
            "Data has been successfully saved to fine_tuning_data_2024-08-28_10-10-10.jsonl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_30791117-2bc6-464f-980b-a393c5e69657\", \"fine_tuning_data_2024-08-28_10-10-10.jsonl\", 142926)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Traning Data - Paul"
      ],
      "metadata": {
        "id": "P5n6s8H7VBnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install necessary libraries\n",
        "!pip install --upgrade gspread pandas gspread-dataframe tiktoken pytz --quiet\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "import gspread\n",
        "from google.auth.transport.requests import Request\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import pandas as pd\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "\n",
        "# Step 3: Authenticate and create the gspread client\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "creds.refresh(Request())\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "# Step 4: Open the Google Sheet by URL\n",
        "sheet = gc.open_by_url('https://docs.google.com/spreadsheets/d/1u_ERBCtzbXa1xRUd2LPEfIAHJlNdmvkv7I_D9CgDobA/edit?usp=sharing')  # Replace with your Google Sheet URL\n",
        "worksheet = sheet.get_worksheet(0)  # Open the first sheet\n",
        "\n",
        "# Step 5: Read data from Google Sheet into pandas DataFrame\n",
        "data = worksheet.get_all_records()\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Print the column names to debug\n",
        "print(\"Column names:\", df.columns)\n",
        "\n",
        "# New system prompt\n",
        "system_prompt = (\"You are a subject matter expert creating a test for the purpose of pre-employment screening for a variety of roles and positions. \"\n",
        "                 \"Your task is to create highly relevant technical and non-technical questions to evaluate the candidates’ skills in a given subject. \"\n",
        "                 \"This is a test and needs to be marked for their overall score, therefore ask questions with a clear correct answer. \"\n",
        "                 \"Do not ask questions that ask the candidate to rate themselves, as this will skew the marking criteria. \"\n",
        "                 \"Ask questions that have a correct answer or logical questions that can be evaluated. \"\n",
        "                 \"Tailor the test to the amount of questions required, always providing the required number of questions, e.g., if requested 30 questions, provide 30 questions and 30 answers. \"\n",
        "                 \"Create questions along with their correct answers for marking.\")\n",
        "\n",
        "# Function to convert a row of the DataFrame to a JSON object for OpenAI fine-tuning\n",
        "def row_to_json(row):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": row['Prompt']},  # User provides the role details\n",
        "        {\"role\": \"assistant\", \"content\": f\"Question: {row['Question']} Answer: {row['Answer']}\"}  # Assistant generates question and answer\n",
        "    ]\n",
        "    return {\"messages\": messages}\n",
        "\n",
        "# Convert the entire DataFrame to a list of JSON objects\n",
        "jsonl_data = df.apply(row_to_json, axis=1).tolist()\n",
        "\n",
        "# Step 6: Format Error Checks Function\n",
        "def check_format_errors(dataset):\n",
        "    format_errors = defaultdict(int)\n",
        "    for ex in dataset:\n",
        "        if not isinstance(ex, dict):\n",
        "            format_errors[\"data_type\"] += 1\n",
        "            continue\n",
        "\n",
        "        messages = ex.get(\"messages\", None)\n",
        "        if not messages:\n",
        "            format_errors[\"missing_messages_list\"] += 1\n",
        "            continue\n",
        "\n",
        "        for message in messages:\n",
        "            if \"role\" not in message or \"content\" not in message:\n",
        "                format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "            if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
        "                format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "            if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
        "                format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "            content = message.get(\"content\", None)\n",
        "            function_call = message.get(\"function_call\", None)\n",
        "\n",
        "            if (not content and not function_call) or not isinstance(content, str):\n",
        "                format_errors[\"missing_content\"] += 1\n",
        "\n",
        "        if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "            format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "    if format_errors:\n",
        "        print(\"Found errors:\")\n",
        "        for k, v in format_errors.items():\n",
        "            print(f\"{k}: {v}\")\n",
        "    else:\n",
        "        print(\"No errors found\")\n",
        "\n",
        "# Step 7: Token Counting Utilities\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribution of {name}:\")\n",
        "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
        "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
        "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
        "\n",
        "# Step 8: Data Warnings and Token Counts Function\n",
        "def data_warnings_and_token_counts(dataset):\n",
        "    n_missing_system = 0\n",
        "    n_missing_user = 0\n",
        "    n_messages = []\n",
        "    convo_lens = []\n",
        "    assistant_message_lens = []\n",
        "\n",
        "    for ex in dataset:\n",
        "        messages = ex[\"messages\"]\n",
        "        if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "            n_missing_system += 1\n",
        "        if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "            n_missing_user += 1\n",
        "        n_messages.append(len(messages))\n",
        "        convo_lens.append(num_tokens_from_messages(messages))\n",
        "        assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "\n",
        "    print(\"Num examples missing system message:\", n_missing_system)\n",
        "    print(\"Num examples missing user message:\", n_missing_user)\n",
        "    print_distribution(n_messages, \"num_messages_per_example\")\n",
        "    print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "    print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "    n_too_long = sum(l > 16385 for l in convo_lens)\n",
        "    print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")\n",
        "\n",
        "# Step 9: Cost Estimation Function\n",
        "def estimate_costs(dataset):\n",
        "    MAX_TOKENS_PER_EXAMPLE = 16385\n",
        "    TARGET_EPOCHS = 3\n",
        "    MIN_TARGET_EXAMPLES = 100\n",
        "    MAX_TARGET_EXAMPLES = 25000\n",
        "    MIN_DEFAULT_EPOCHS = 1\n",
        "    MAX_DEFAULT_EPOCHS = 25\n",
        "\n",
        "    n_epochs = TARGET_EPOCHS\n",
        "    n_train_examples = len(dataset)\n",
        "    if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "        n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "    elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "        n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "    convo_lens = [num_tokens_from_messages(ex[\"messages\"]) for ex in dataset]\n",
        "    n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "    print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
        "    print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "    print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
        "\n",
        "# Step 10: Execute the functions with the dataset\n",
        "check_format_errors(jsonl_data)\n",
        "data_warnings_and_token_counts(jsonl_data)\n",
        "estimate_costs(jsonl_data)\n",
        "\n",
        "# Step 11: Save the JSONL data to a file with date and time in the filename\n",
        "australia_timezone = pytz.timezone('Australia/Sydney')\n",
        "current_datetime = datetime.now(australia_timezone).strftime('%Y-%m-%d_%H-%M-%S')\n",
        "output_file = f'fine_tuning_data_{current_datetime}.jsonl'\n",
        "\n",
        "with open(output_file, 'w') as f:\n",
        "    for entry in jsonl_data:\n",
        "        json.dump(entry, f)\n",
        "        f.write('\\n')\n",
        "\n",
        "print(f'Data has been successfully saved to {output_file}')\n",
        "\n",
        "# Optional: Download the JSONL file to your local machine\n",
        "from google.colab import files\n",
        "files.download(output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "Vdta3uUSVCRY",
        "outputId": "58ba44de-3459-4457-a184-a68bdefd3098"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names: Index(['Prompt', 'Question', 'Answer', 'Difficulty'], dtype='object')\n",
            "No errors found\n",
            "Num examples missing system message: 0\n",
            "Num examples missing user message: 0\n",
            "\n",
            "#### Distribution of num_messages_per_example:\n",
            "min / max: 3, 3\n",
            "mean / median: 3.0, 3.0\n",
            "p5 / p95: 3.0, 3.0\n",
            "\n",
            "#### Distribution of num_total_tokens_per_example:\n",
            "min / max: 207, 293\n",
            "mean / median: 232.48936170212767, 230.0\n",
            "p5 / p95: 221.6, 250.0\n",
            "\n",
            "#### Distribution of num_assistant_tokens_per_example:\n",
            "min / max: 18, 96\n",
            "mean / median: 36.04255319148936, 32.0\n",
            "p5 / p95: 22.6, 53.0\n",
            "\n",
            "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n",
            "Dataset has ~10927 tokens that will be charged for during training\n",
            "By default, you'll train for 3 epochs on this dataset\n",
            "By default, you'll be charged for ~32781 tokens\n",
            "Data has been successfully saved to fine_tuning_data_2024-08-28_10-18-29.jsonl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4fcc63d7-837c-483b-81e7-6d6956cbc768\", \"fine_tuning_data_2024-08-28_10-18-29.jsonl\", 58417)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}